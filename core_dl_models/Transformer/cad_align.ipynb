{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我想做一个模型，基于transformer架构, 具体的细节:\n",
    "1. 输入三个2维点集 A,B,C\n",
    "2. 每个点构成一个2维token，坐标x, y\n",
    "3. 每个点集长度固定为100,不够则padding (0,0) token;\n",
    "4. 由transformer作为主体模块的编码器E，输入一个点集S（固定长度为100的2维token),输出一个50维的embedding向量T\n",
    "5. 将 A\\B\\C 3个点集分别输入E得到对应的3个embedding向量Sa\\Sb\\Sc\n",
    "6. 最终的损失函数为： 最小化 Sa - (Sb + Sc)及正则化惩罚项\n",
    "7. 整体思路如上，该模型需要学习一个点集的编码方式，使得 原始点集 A=B+C(B和C是关于A的互补点集)的降维embedding向量也能反映这种关系\n",
    "\n",
    "请提出好的建议并给出代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        # 定义一个线性层，用于将输入的2D点集映射到hidden_dim维度的向量\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        # 定义一个Transformer编码器，用于将输入的向量序列编码成一个向量\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(hidden_dim, num_heads, hidden_dim, dropout),\n",
    "            num_layers\n",
    "        )\n",
    "        # 定义一个线性层，用于将Transformer编码器的输出映射到output_dim维度的向量\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将输入的2D点集映射到hidden_dim维度的向量\n",
    "        x = self.embedding(x)\n",
    "        # 将batch_size放在第二个维度上\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # 将输入的向量序列编码成一个向量\n",
    "        x = self.transformer(x)\n",
    "        # 将batch_size放回第一个维度上\n",
    "        x = x.permute(1, 0, 2)\n",
    "        # 将Transformer编码器的输出映射到output_dim维度的向量\n",
    "        x = self.fc(x[:, 0, :])\n",
    "        return x\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 定义一个包含三个Transformer编码器的模型\n",
    "        self.encoder = TransformerEncoder(2, 128, 50, 6, 8, 0.1)\n",
    "\n",
    "    def forward(self, A, B, C):\n",
    "        # 对A进行一个随机的仿射变换，得到点集B'\n",
    "        B_ = A @ torch.randn(2, 2) + torch.randn(1, 2)\n",
    "        # 随机去掉B'其中一些点（加入点集C）\n",
    "        indices = torch.randperm(B_.shape[0])[:int(B_.shape[0] * 0.9)]\n",
    "        C_ = B_[indices]\n",
    "        B_ = B_[~indices]\n",
    "        # 随机增加一些围绕B'中剩下点的噪声点（满足一定比率10%左右）\n",
    "        noise = torch.randn(int(B_.shape[0] * 0.1), 2) + B_.mean(dim=0)\n",
    "        B_ = torch.cat([B_, noise], dim=0)\n",
    "        C_ = torch.cat([C_, noise], dim=0)\n",
    "        # 将三个点集分别输入到Transformer编码器中，得到对应的三个embedding向量\n",
    "        Sa = self.encoder(A)\n",
    "        Sb = self.encoder(B_)\n",
    "        Sc = self.encoder(C_)\n",
    "        # 计算损失函数\n",
    "        loss = torch.norm(Sa - (Sb + Sc)) + 0.01 * (torch.norm(Sa) + torch.norm(Sb) + torch.norm(Sc))\n",
    "        return loss\n",
    "\n",
    "# 构造数据集\n",
    "data = torch.randn(100, 2)\n",
    "dataset = MyDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 定义模型和优化器\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        A = batch\n",
    "        B = None\n",
    "        C = None\n",
    "        loss = model(A, B, C)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "# 测试模型\n",
    "A = torch.randn(100, 2)\n",
    "B = None\n",
    "C = None\n",
    "with torch.no_grad():\n",
    "    Sa = model.encoder(A)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
